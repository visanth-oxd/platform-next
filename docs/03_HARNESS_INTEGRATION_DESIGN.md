# Harness CD Integration - Detailed Design

## Executive Summary

This document describes how Harness CD orchestrates deployments, fetches manifests from the config repo, and manages multi-cluster deployments.

**Key Points**:
- Per-service pipelines (100+ pipelines)
- Runtime image tag injection
- Cross-org manifest fetching
- Multi-cluster delegate architecture
- GitOps-compliant (deploys from Git)

---

## 1. Why Harness for CD?

### Rationale

**Alternatives Considered**:
| Tool | Pros | Cons | Verdict |
|------|------|------|---------|
| **ArgoCD** | Free, GitOps-native | Weak approvals, new tool | âŒ Rejected |
| **Flux** | Free, declarative | No UI, limited controls | âŒ Rejected |
| **GitHub Actions** | Integrated, free | Not enterprise-grade | âŒ Rejected |
| **Harness** | Enterprise features, familiar | Cost, vendor lock-in | âœ… **Selected** |

**Why Harness Won**:
1. âœ… **Already deployed** - Teams know it
2. âœ… **Enterprise approvals** - Multi-gate, role-based
3. âœ… **Canary/Blue-Green** - Production-grade deployment strategies
4. âœ… **Audit trail** - Compliance requirements met
5. âœ… **Multi-cluster** - Native support via delegates
6. âœ… **RBAC** - Pipeline-level permissions
7. âœ… **Integrations** - PagerDuty, Slack, Jira, ServiceNow
8. âœ… **Runtime inputs** - Image tag at deployment time

**Cost Justification**:
- Harness: ~$120K/year for 100 services
- Engineer time saved: ~200 hours/month (worth ~$50K/month)
- **ROI**: Positive within 3 months

---

## 2. Pipeline Architecture

### Per-Service Pipeline Model

**Why Per-Service (not mono-pipeline)?**

| Aspect | Mono-Pipeline | Per-Service Pipelines | Winner |
|--------|---------------|----------------------|--------|
| **Isolation** | âŒ Shared execution | âœ… Independent | Per-service |
| **Scalability** | âŒ Bottleneck | âœ… Parallel (20+) | Per-service |
| **RBAC** | âŒ All-or-nothing | âœ… Granular | Per-service |
| **Rollback** | âŒ Affects all | âœ… Per-service | Per-service |
| **Customization** | âŒ Hard | âœ… Easy | Per-service |

**Decision**: One pipeline per service

---

### Pipeline Template Structure

**Location**: `harness-pipelines/templates/api-pipeline-template.yaml`

**Sections**:
1. **Metadata** - Name, ID, tags
2. **Variables** - Runtime inputs (imageTag, environment, region)
3. **Stages** - Deployment stages (int â†’ pre â†’ prod)
4. **Steps** - K8s operations (apply, rolling deploy, verify)
5. **Triggers** - Git webhook, API webhook

### Complete Pipeline Template

```yaml
pipeline:
  name: "{{SERVICE_NAME}}-cd"
  identifier: "{{SERVICE_NAME}}_cd"
  projectIdentifier: platform
  orgIdentifier: default
  
  tags:
    service: "{{SERVICE_NAME}}"
    archetype: "{{ARCHETYPE}}"
    team: "{{TEAM}}"
    managed-by: platform-next
  
  # ================================================
  # VARIABLES - Runtime Inputs
  # ================================================
  variables:
    - name: imageTag
      type: String
      description: "Docker image tag to deploy (e.g., v2.3.1, latest, abc123)"
      required: true
      value: <+input>
      default: latest
    
    - name: targetEnvironment
      type: String
      description: "Environment to deploy to"
      required: true
      value: <+input>.allowedValues(int-stable,pre-stable,prod)
      default: int-stable
    
    - name: targetRegion
      type: String
      description: "Region to deploy to"
      required: true
      value: <+input>.allowedValues(euw1,euw2)
      default: euw1
  
  # ================================================
  # STAGES - Deployment Stages
  # ================================================
  stages:
    # ------------------------------------------------
    # Stage 1: Deploy to Selected Environment
    # ------------------------------------------------
    - stage:
        name: Deploy to <+pipeline.variables.targetEnvironment>
        identifier: deploy_service
        type: Deployment
        
        # Stage execution condition
        when:
          pipelineStatus: Success
          condition: <+pipeline.variables.targetEnvironment> != ""
        
        spec:
          deploymentType: Kubernetes
          
          # ================================================
          # SERVICE DEFINITION
          # ================================================
          service:
            serviceRef: "{{SERVICE_NAME}}"
            serviceDefinition:
              type: Kubernetes
              spec:
                # Variables used in manifests
                variables:
                  - name: serviceName
                    type: String
                    value: "{{SERVICE_NAME}}"
                  
                  - name: imageTag
                    type: String
                    value: <+pipeline.variables.imageTag>
                  
                  - name: environment
                    type: String
                    value: <+pipeline.variables.targetEnvironment>
                  
                  - name: region
                    type: String
                    value: <+pipeline.variables.targetRegion>
                
                # ================================================
                # MANIFESTS - Fetch from Config Repo (GitOps)
                # ================================================
                manifests:
                  - manifest:
                      identifier: k8s_manifests_gitops
                      type: K8sManifest
                      spec:
                        store:
                          type: Github
                          spec:
                            # Cross-org connector to platform-next
                            connectorRef: github_platform_next_cross_org
                            gitFetchType: Branch
                            branch: main
                            paths:
                              # Dynamic path based on runtime inputs
                              - generated/{{SERVICE_NAME}}/<+pipeline.variables.targetEnvironment>/<+pipeline.variables.targetRegion>/manifests.yaml
                        
                        # Don't skip versioning (for rollback)
                        skipResourceVersioning: false
          
          # ================================================
          # ENVIRONMENT & INFRASTRUCTURE
          # ================================================
          environment:
            # Dynamic environment ref
            environmentRef: <+pipeline.variables.targetEnvironment>
            deployToAll: false
            
            # Infrastructure definition per env/region
            infrastructureDefinitions:
              - identifier: <+pipeline.variables.targetEnvironment>_<+pipeline.variables.targetRegion>_k8s
                inputs:
                  type: KubernetesDirect
                  spec:
                    # Kubernetes connector (per cluster)
                    connectorRef: k8s_<+pipeline.variables.targetEnvironment>_<+pipeline.variables.targetRegion>
                    
                    # Namespace (dynamic)
                    namespace: <+pipeline.variables.targetEnvironment>-{{SERVICE_NAME}}-<+pipeline.variables.targetRegion>
                    
                    # Release name
                    releaseName: {{SERVICE_NAME}}
                    
                    # âœ… KEY: Delegate selection (multi-cluster)
                    delegateSelectors:
                      - harness-delegate-<+pipeline.variables.targetEnvironment>-<+pipeline.variables.targetRegion>
          
          # ================================================
          # EXECUTION STEPS
          # ================================================
          execution:
            steps:
              # ============================================
              # Conditional Approval for Production
              # ============================================
              - step:
                  type: HarnessApproval
                  name: Production Approval Gate
                  identifier: prod_approval
                  
                  # Only run for production
                  when:
                    stageStatus: Success
                    condition: <+pipeline.variables.targetEnvironment> == "prod"
                  
                  spec:
                    approvalMessage: |
                      ## Production Deployment Request
                      
                      **Service**: {{SERVICE_NAME}}
                      **Image Tag**: <+pipeline.variables.imageTag>
                      **Region**: <+pipeline.variables.targetRegion>
                      **Requester**: <+pipeline.triggeredBy.email>
                      
                      **Required Approvers**: 2
                      - 1 from {{TEAM}}
                      - 1 from Platform Team
                      
                      **Please provide**:
                      - Change ticket ID
                      - Rollback plan
                      - Business justification
                    
                    approvers:
                      userGroups:
                        - {{TEAM}}
                        - platform_team
                      minimumCount: 2
                      disallowPipelineExecutor: true
                    
                    approverInputs:
                      - name: change_ticket
                        type: String
                        label: "Change Ticket ID"
                        required: true
                      
                      - name: rollback_plan
                        type: String
                        label: "Rollback Plan"
                        required: true
                      
                      - name: business_justification
                        type: String
                        label: "Business Justification"
                        required: false
                  
                  timeout: 7d
                  failureStrategies:
                    - onFailure:
                        errors:
                          - Timeout
                        action:
                          type: Abort
              
              # ============================================
              # Production Change Window Check
              # ============================================
              - step:
                  type: Policy
                  name: Check Production Change Window
                  identifier: change_window_policy
                  
                  when:
                    stageStatus: Success
                    condition: <+pipeline.variables.targetEnvironment> == "prod"
                  
                  spec:
                    policySets:
                      - prod_change_window
                    type: Custom
                    policySpec:
                      payload: |
                        {
                          "environment": "prod",
                          "currentTime": "<+currentTime>",
                          "dayOfWeek": "<+currentDate.dayOfWeek>",
                          "service": "{{SERVICE_NAME}}"
                        }
                  
                  timeout: 1m
                  failureStrategies:
                    - onFailure:
                        errors:
                          - PolicyEvaluationFailure
                        action:
                          type: Abort
              
              # ============================================
              # Fetch and Apply Manifests (K8s Native)
              # ============================================
              - step:
                  type: K8sApply
                  name: Apply Kubernetes Manifests
                  identifier: k8s_apply
                  
                  spec:
                    # Fetch from Git (GitOps)
                    filePaths:
                      - generated/{{SERVICE_NAME}}/<+pipeline.variables.targetEnvironment>/<+pipeline.variables.targetRegion>/manifests.yaml
                    
                    # Dry run first
                    skipDryRun: false
                    
                    # Wait for steady state
                    skipSteadyStateCheck: false
                    
                    # Command flags
                    commandFlags:
                      - --prune
                      - --selector=app={{SERVICE_NAME}}
                  
                  timeout: 10m
                  failureStrategies:
                    - onFailure:
                        errors:
                          - AllErrors
                        action:
                          type: StageRollback
              
              # ============================================
              # Rolling Deployment (K8s Native)
              # ============================================
              - step:
                  type: K8sRollingDeploy
                  name: Rolling Deployment
                  identifier: rolling_deploy
                  
                  spec:
                    skipDryRun: false
                    pruningEnabled: false
                  
                  timeout: 10m
              
              # ============================================
              # Canary Deployment (Production Only)
              # ============================================
              - stepGroup:
                  name: Canary Deployment
                  identifier: canary_group
                  
                  when:
                    stageStatus: Success
                    condition: <+pipeline.variables.targetEnvironment> == "prod"
                  
                  steps:
                    - step:
                        type: K8sCanaryDeploy
                        name: Deploy Canary (10%)
                        identifier: canary_10
                        spec:
                          instanceSelection:
                            type: Count
                            spec:
                              count: 1
                        timeout: 10m
                    
                    - step:
                        type: Verify
                        name: Verify Canary Metrics
                        identifier: verify_canary_10
                        spec:
                          type: Canary
                          spec:
                            sensitivity: MEDIUM
                            duration: 5m
                            deploymentTag: <+service.name>-<+pipeline.sequenceId>
                        timeout: 5m
                    
                    - step:
                        type: K8sCanaryDeploy
                        name: Deploy Canary (50%)
                        identifier: canary_50
                        spec:
                          instanceSelection:
                            type: Percentage
                            spec:
                              percentage: 50
                        timeout: 10m
                    
                    - step:
                        type: Verify
                        name: Verify 50% Metrics
                        identifier: verify_canary_50
                        spec:
                          type: Canary
                          duration: 10m
                        timeout: 10m
                    
                    - step:
                        type: K8sCanaryDelete
                        name: Complete Rollout (100%)
                        identifier: canary_complete
                        timeout: 5m
              
              # ============================================
              # Health Check (K8s Native)
              # ============================================
              - step:
                  type: K8sBlueGreenDeploy
                  name: Verify Deployment Health
                  identifier: verify_health
                  
                  spec:
                    skipDryRun: false
                  
                  timeout: 5m
            
            # ================================================
            # ROLLBACK STEPS
            # ================================================
            rollbackSteps:
              - step:
                  type: K8sRollingRollback
                  name: Rollback to Previous Version
                  identifier: rollback
                  timeout: 5m
        
        # Stage failure handling
        failureStrategies:
          - onFailure:
              errors:
                - AllErrors
              action:
                type: StageRollback
  
  # ================================================
  # TRIGGERS - How Pipeline Gets Invoked
  # ================================================
  triggers:
    # Trigger 1: API/Webhook (from app build)
    - trigger:
        name: App Image Built
        identifier: app_image_trigger
        enabled: true
        source:
          type: Webhook
          spec:
            type: Custom
            spec:
              payloadConditions:
                - key: service_name
                  operator: Equals
                  value: "{{SERVICE_NAME}}"
                - key: image_tag
                  operator: NotNull
                  value: ""
                - key: target_env
                  operator: NotNull
                  value: ""
        
        # Map webhook payload to pipeline variables
        inputYaml: |
          pipeline:
            identifier: {{SERVICE_NAME}}_cd
            variables:
              imageTag: <+trigger.payload.image_tag>
              targetEnvironment: <+trigger.payload.target_env>
              targetRegion: <+trigger.payload.target_region>
    
    # Trigger 2: Manual (from Backstage/UI)
    # (No configuration needed, always available)
```

---

## 3. Multi-Cluster Architecture

### The Challenge

**Different environments are in different clusters and different networks**:

```
Environment   | Cluster       | Network CIDR | Access
--------------|---------------|--------------|-------------
int-stable    | k8s-int.cloud | 10.1.0.0/16  | VPN/Direct
pre-stable    | k8s-pre.cloud | 10.2.0.0/16  | VPN/Direct
prod          | k8s-prod.cloud| 10.3.0.0/16  | Bastion only
```

**Implication**: Cannot use single Harness delegate for all clusters

---

### Solution: Delegate Per Cluster

**Architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Harness SaaS Platform                                   â”‚
â”‚ https://app.harness.io                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚            â”‚
             â”‚            â”‚            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”   â”Œâ”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Delegate A â”‚   â”‚Delegate Bâ”‚   â”‚ Delegate C â”‚
    â”‚ int-stable â”‚   â”‚pre-stableâ”‚   â”‚    prod    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                                              â”‚
â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚ Cluster A       â”‚  â”‚ Cluster B    â”‚  â”‚ Cluster C     â”‚
â”‚ (int-stable)    â”‚  â”‚ (pre-stable) â”‚  â”‚ (prod)        â”‚
â”‚ Network A       â”‚  â”‚ Network B    â”‚  â”‚ Network C     â”‚
â”‚ 10.1.0.0/16     â”‚  â”‚ 10.2.0.0/16  â”‚  â”‚ 10.3.0.0/16   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Delegate Deployment

**Deploy one delegate per cluster**:

```bash
# Int-Stable Cluster
kubectl create namespace harness-delegate-ng

helm install harness-delegate harness-delegate/harness-delegate-ng \
  --namespace harness-delegate-ng \
  --set delegateName=harness-delegate-int-euw1 \
  --set delegateProfile=int-stable \
  --set accountId=${HARNESS_ACCOUNT_ID} \
  --set delegateToken=${HARNESS_DELEGATE_TOKEN} \
  --set managerEndpoint=https://app.harness.io/gratis \
  --set tags="int-stable,euw1,network-a"

# Pre-Stable Cluster
# (Same, but different name and tags)
helm install harness-delegate harness-delegate/harness-delegate-ng \
  --namespace harness-delegate-ng \
  --set delegateName=harness-delegate-pre-euw1 \
  --set tags="pre-stable,euw1,network-b"

# Prod Cluster
helm install harness-delegate harness-delegate/harness-delegate-ng \
  --namespace harness-delegate-ng \
  --set delegateName=harness-delegate-prod-euw1 \
  --set tags="prod,euw1,network-c"
```

### Infrastructure Definitions

**In Harness UI**: Setup â†’ Environments â†’ {env} â†’ Infrastructure Definitions

```yaml
# Int-Stable EUW1
name: int-stable-euw1
identifier: int_stable_euw1_k8s
environmentRef: int_stable
type: KubernetesDirect
spec:
  connectorRef: k8s_int_stable_euw1
  namespace: <+service.name>-<+env.name>
  releaseName: <+service.name>
  delegateSelectors:
    - harness-delegate-int-euw1  # â† Specific delegate

# Pre-Stable EUW1
name: pre-stable-euw1
identifier: pre_stable_euw1_k8s
environmentRef: pre_stable
spec:
  connectorRef: k8s_pre_stable_euw1
  delegateSelectors:
    - harness-delegate-pre-euw1  # â† Different delegate

# Prod EUW1
name: prod-euw1
identifier: prod_euw1_k8s
environmentRef: prod
spec:
  connectorRef: k8s_prod_euw1
  delegateSelectors:
    - harness-delegate-prod-euw1  # â† Production delegate
```

**Why This Works**:
- Each delegate runs in its cluster (can access K8s API locally)
- Network isolation maintained (delegate doesn't cross networks)
- Harness routes commands to correct delegate based on selector

---

## 4. Cross-Org Integration

### The Challenge

**Harness pipelines in different GitHub org than manifests**:
- Pipelines: `company-harness/harness-pipelines`
- Manifests: `company/platform-next`

**Need**: Harness must fetch manifests from different GitHub org

### Solution: Cross-Org GitHub Connector

**Setup in Harness**:

1. **Create GitHub Connector**
   - Name: `github_platform_next_cross_org`
   - URL: `https://github.com/company/platform-next`
   - Connection Type: HTTP
   - Authentication: Personal Access Token

2. **GitHub PAT Requirements**:
   - Create service account: `harness-bot@company.com`
   - Generate PAT with `repo` scope (read access)
   - Store in Harness Secrets: `github_platform_next_pat`
   - Add service account to `company/platform-next` repo (Read permission)

3. **Test Connection**:
   ```
   Harness â†’ Connectors â†’ github_platform_next_cross_org
   â†’ Test Connection â†’ Should succeed
   ```

4. **Use in Pipeline**:
   ```yaml
   manifests:
     - manifest:
         spec:
           store:
             type: Github
             spec:
               connectorRef: github_platform_next_cross_org  # â† Cross-org
               branch: main
               paths:
                 - generated/payment-service/prod/euw1/manifests.yaml
   ```

---

## 5. Image Tag Injection

### The Problem

**Manifests in Git have placeholder**:
```yaml
# generated/payment-service/prod/euw1/manifests.yaml
spec:
  containers:
    - name: app
      image: gcr.io/project/payment-service:PLACEHOLDER_TAG
```

**Need**: Replace `PLACEHOLDER_TAG` with actual image tag at deployment time

### Solution: Harness Image Replacement

**Method 1: Via Service Definition Variables**

```yaml
# In pipeline template
service:
  serviceDefinition:
    spec:
      variables:
        - name: imageTag
          value: <+pipeline.variables.imageTag>
      
      artifacts:
        primary:
          primaryArtifactRef: <+input>
          sources:
            - identifier: docker_image
              type: DockerRegistry
              spec:
                connectorRef: gcr_connector
                imagePath: gcr.io/project/{{SERVICE_NAME}}
                tag: <+pipeline.variables.imageTag>
```

**Method 2: Via Kustomize Images Transformer**

```yaml
# Generated by script in kustomization.yaml
images:
  - name: placeholder
    newName: gcr.io/project/payment-service
    newTag: PLACEHOLDER_TAG  # Will be replaced

# Harness replaces at runtime
# Final result: gcr.io/project/payment-service:v2.3.1
```

---

## 6. Pipeline Execution Flow

### Deployment Sequence

```
Developer triggers pipeline
  â”œâ”€ Inputs: imageTag=v2.3.1, targetEnvironment=prod, targetRegion=euw1
  â”‚
  â”œâ”€ Step 1: Production Approval Gate
  â”‚   â”œâ”€ Check if env=prod â†’ Yes
  â”‚   â”œâ”€ Wait for 2 approvers
  â”‚   â”œâ”€ Collect: change_ticket, rollback_plan
  â”‚   â””â”€ Approved âœ“
  â”‚
  â”œâ”€ Step 2: Change Window Policy Check
  â”‚   â”œâ”€ Evaluate OPA policy
  â”‚   â”œâ”€ Check current time: Monday 14:00 UTC
  â”‚   â”œâ”€ Change window: Mon-Thu 10:00-16:00 â†’ Allowed âœ“
  â”‚   â””â”€ Pass
  â”‚
  â”œâ”€ Step 3: Fetch Manifests from Git
  â”‚   â”œâ”€ Connector: github_platform_next_cross_org
  â”‚   â”œâ”€ Branch: main
  â”‚   â”œâ”€ Path: generated/payment-service/prod/euw1/manifests.yaml
  â”‚   â”œâ”€ Content fetched âœ“
  â”‚   â””â”€ Stored in Harness workspace
  â”‚
  â”œâ”€ Step 4: K8s Apply
  â”‚   â”œâ”€ Select delegate: harness-delegate-prod-euw1
  â”‚   â”œâ”€ Dry run: Success
  â”‚   â”œâ”€ Apply to cluster
  â”‚   â”œâ”€ Wait for steady state
  â”‚   â””â”€ Complete âœ“
  â”‚
  â”œâ”€ Step 5: Canary 10%
  â”‚   â”œâ”€ Deploy 1 pod with new version
  â”‚   â”œâ”€ Monitor metrics (5 min)
  â”‚   â”œâ”€ Check error rate, latency
  â”‚   â””â”€ Healthy âœ“
  â”‚
  â”œâ”€ Step 6: Canary 50%
  â”‚   â”œâ”€ Deploy 50% pods with new version
  â”‚   â”œâ”€ Monitor metrics (10 min)
  â”‚   â””â”€ Healthy âœ“
  â”‚
  â”œâ”€ Step 7: Complete Rollout
  â”‚   â”œâ”€ Deploy to all pods
  â”‚   â”œâ”€ Delete canary resources
  â”‚   â””â”€ Complete âœ“
  â”‚
  â””â”€ Step 8: Verify Health
      â”œâ”€ Check pod status: 6/6 running
      â”œâ”€ Check readiness probes: All passing
      â””â”€ Deployment successful âœ“
```

**Total Time**: 5-30 minutes depending on environment and canary duration

---

## 7. Benefits & Rationale

### Why Harness CD?

| Requirement | Harness Solution | Alternative |
|-------------|------------------|-------------|
| **Multi-cluster** | Delegates per cluster | ArgoCD: Limited |
| **Approvals** | Native multi-gate | GitHub: Manual |
| **Canary** | Built-in | ArgoCD: Complex |
| **Runtime inputs** | Image tag injection | ArgoCD: Not supported |
| **Audit** | Full trail | Git: Limited |
| **RBAC** | Pipeline-level | Repo-level: Coarse |
| **Integrations** | PagerDuty, Slack, Jira | Manual setup |

### Why Not GitOps-Only?

**GitOps (ArgoCD/Flux) Limitations**:
- âŒ No runtime inputs (image tag must be in Git)
- âŒ Weak approval workflows
- âŒ Limited canary support
- âŒ No enterprise integrations

**Harness Advantages**:
- âœ… Runtime inputs (decouple app from config)
- âœ… Enterprise approvals (multi-gate, roles)
- âœ… Native canary (traffic splitting)
- âœ… Rich integrations (alerts, tickets)

### Hybrid Approach: Best of Both

```
Config Changes â†’ GitOps (declarative, in Git)
App Deployments â†’ Harness (runtime inputs, approvals)
```

**Result**: GitOps benefits + Enterprise controls

---

## 8. Integration with Backstage

### How They Connect

**Backstage â†’ Harness Integration**:

1. **Links in Catalog Entity**
   ```yaml
   # backstage/catalog/payment-service.yaml
   metadata:
     annotations:
       harness.io/pipeline-url: https://harness.company.com/.../payment-service-cd
     links:
       - url: https://harness.company.com/.../pipelines/payment-service-cd
         title: Deploy in Harness
   ```

2. **Quick Deploy Action**
   ```yaml
   # Backstage custom action
   - id: deploy-to-env
     name: Deploy to Environment
     action: harness:trigger-pipeline
     input:
       pipelineId: payment-service-cd
       variables:
         imageTag: <+input>
         targetEnvironment: int-stable
   ```

3. **Deployment History Widget**
   ```yaml
   # Backstage service page shows Harness deployments
   # Via @backstage-community/plugin-harness
   
   Recent Deployments:
     âœ“ v2.3.1 â†’ prod (2 hours ago) - Success
     âœ“ v2.3.0 â†’ prod (1 day ago) - Success
     âœ— v2.2.9 â†’ prod (3 days ago) - Failed
   ```

### Integration with Config Repo

**Harness â†’ Platform-Next Integration**:

```yaml
# Pipeline fetches manifests from Git
manifests:
  - manifest:
      spec:
        store:
          type: Github
          spec:
            connectorRef: github_platform_next_cross_org
            branch: main
            paths:
              - generated/{{SERVICE_NAME}}/prod/euw1/manifests.yaml
```

**Key Points**:
- âœ… Always fetches latest from `main` branch
- âœ… Config changes auto-applied (GitOps)
- âœ… No manual sync needed
- âœ… Manifests are source of truth

---

## 9. Pipeline Lifecycle Management

### Pipeline Creation

**Trigger**: New service added via Backstage

**Flow**:
```
Backstage template executes
  â†“
Calls Pipeline Orchestrator API
  â†“
Pipeline Orchestrator:
  1. Fetch template from harness-pipelines repo
  2. Replace {{SERVICE_NAME}}, {{TEAM}}, etc.
  3. Call Harness API to create pipeline
  4. Commit pipeline YAML to harness-pipelines repo
  5. Return pipeline URL
```

**Result**: Pipeline ready to use in ~2 minutes

### Pipeline Updates

**When**: Template updated (new features, policy changes)

**Flow**:
```
Update template in harness-pipelines/templates/
  â†“
Run bulk update script
  â†“
For each pipeline:
  1. Regenerate from template
  2. Call Harness API to update
  3. Commit updated YAML
  â†“
All pipelines updated
```

**Script**: `harness-pipelines/scripts/update-all-pipelines.sh`

```bash
#!/bin/bash
# Update all pipelines from template

for SERVICE in $(yq eval '.services[].name' ../platform-next/kustomize/catalog/services.yaml); do
  ARCHETYPE=$(yq eval ".services[] | select(.name == \"$SERVICE\") | .archetype" ../platform-next/kustomize/catalog/services.yaml)
  
  # Regenerate pipeline
  ./generate-pipeline.sh $SERVICE $ARCHETYPE
  
  # Update in Harness
  harness pipeline update --file pipelines/${SERVICE}-cd.yaml
done
```

### Pipeline Deletion

**Trigger**: Service decommissioned

**Flow**:
```
Remove service from catalog
  â†“
Call Pipeline Orchestrator API DELETE
  â†“
Pipeline Orchestrator:
  1. Call Harness API to delete pipeline
  2. Move pipeline YAML to archived/
  3. Remove from harness-pipelines repo
```

---

## 10. Security & Compliance

### Security Controls

| Control | Implementation | Enforcement |
|---------|----------------|-------------|
| **No shell scripts** | Only K8s native steps | Policy blocked |
| **Image scanning** | Harness policy (before deploy) | OPA |
| **Resource limits** | Validated in CI | JSON schema |
| **Network policies** | Applied via components | K8s |
| **Pod security** | Security context in archetypes | K8s admission |
| **RBAC** | Pipeline-level permissions | Harness |

### Compliance Features

**Audit Trail**:
- Git history (who changed what config)
- Harness execution history (who deployed what)
- Approval records (who approved prod deploys)
- Change tickets (linked to deployments)

**Change Management**:
- Change window enforcement (OPA policy)
- Mandatory approvals (2+ for prod)
- Rollback plan required
- Business justification captured

---

## 11. Operational Procedures

### Deploy New Version

**Via Backstage**:
1. Navigate to service page
2. Click "Deploy to Environment"
3. Enter image tag
4. Redirects to Harness

**Via Harness Console**:
1. Navigate to Pipelines â†’ {service}-cd
2. Click "Run Pipeline"
3. Enter runtime inputs:
   - imageTag: v2.3.1
   - targetEnvironment: prod
   - targetRegion: euw1
4. Click "Run"
5. Approve when prompted (if prod)

**Via API/Webhook**:
```bash
# From app CI/CD
curl -X POST \
  "https://app.harness.io/gateway/ng/api/webhook/..." \
  -H "x-api-key: $HARNESS_WEBHOOK_TOKEN" \
  -d '{
    "service_name": "payment-service",
    "image_tag": "v2.3.1",
    "target_env": "prod",
    "target_region": "euw1"
  }'
```

### Rollback

**Method 1: Deploy Previous Version**
```
Harness â†’ Run Pipeline
  â†’ imageTag: v2.3.0 (previous version)
  â†’ Deploy
```

**Method 2: Harness Rollback**
```
Harness â†’ Execution History
  â†’ Select failed deployment
  â†’ Click "Rollback"
  â†’ Automatic rollback to previous version
```

**Method 3: Git Revert (Config Issue)**
```
git revert <bad-commit>
git push
CI regenerates manifests
Next deploy uses reverted config
```

---

## Summary

### Three-Component Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Backstage (Developer Interface)                      â”‚
â”‚    - Self-service onboarding                            â”‚
â”‚    - Service catalog and discovery                      â”‚
â”‚    - Links to Harness pipelines                         â”‚
â”‚    - Pod status and monitoring                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Creates service entry
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Platform-Next (Config Management)                    â”‚
â”‚    - Kustomize archetypes and components                â”‚
â”‚    - Service catalog (single file)                      â”‚
â”‚    - Manifest generation (CI)                           â”‚
â”‚    - Git as source of truth (GitOps)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Fetches manifests
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Harness (Deployment Orchestration)                   â”‚
â”‚    - Per-service pipelines                              â”‚
â”‚    - Runtime image tag injection                        â”‚
â”‚    - Multi-cluster delegates                            â”‚
â”‚    - Enterprise approvals and controls                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Developer Journey

**Onboard** (once): Backstage form â†’ 5 min â†’ Service ready
**Deploy** (recurring): Harness console â†’ Enter image tag â†’ Deploy â†’ 5-10 min
**Monitor**: Backstage â†’ View pods, logs, metrics

**Platform handles**: Config generation, GitOps, approvals, multi-cluster

---

**This is the complete, production-ready architecture!** ğŸš€
